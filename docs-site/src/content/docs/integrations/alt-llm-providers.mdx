---
title: "Alternative LLM Providers"
description: >
  Use Claude Code with open-weight LLMs via
  Anthropic-compatible API providers like Kimi K2,
  GLM-4.5, DeepSeek, and MiniMax M2.1.
---

import { Tabs, TabItem } from '@astrojs/starlight/components';

:::note[Installation]
Part of the `claude-code-tools` package.
See [Quick Start](../../getting-started/) for installation.
:::

## Overview

You can use Claude Code with alternative LLMs served
via Anthropic-compatible APIs. Each provider exposes
an endpoint that speaks the Anthropic `/v1/messages`
format, so Claude Code works without any code changes
-- you just set a few environment variables.

The shell functions below use **subshells** `( ... )`
so the environment variables do not leak into your
main shell session. This means you can run multiple
instances of Claude Code simultaneously, each using
a different LLM.

## API Key Setup

Before using any provider, export the corresponding
API key in your shell (or add it to your
`.zshrc` / `.bashrc`):

```bash
export KIMI_API_KEY="your-kimi-key"
export Z_API_KEY="your-z-key"
export DEEPSEEK_API_KEY="your-deepseek-key"
export MINIMAX_API_KEY="your-minimax-key"
```

## Shell Functions

Add the function for the provider(s) you want to use
to your `~/.zshrc` or `~/.bashrc`, then use it
exactly like the `claude` command.

<Tabs>
<TabItem label="Kimi K2">

[Kimi K2](https://platform.moonshot.ai/) from
Moonshot AI.

```bash
kimi() {
    (
        export ANTHROPIC_BASE_URL=https://api.moonshot.ai/anthropic
        export ANTHROPIC_AUTH_TOKEN=$KIMI_API_KEY
        claude "$@"
    )
}
```

**Usage:**

```bash
kimi                   # start interactive session
kimi -p "explain X"    # one-shot prompt
```

</TabItem>
<TabItem label="GLM-4.5 (zai)">

[GLM-4.5](https://api.z.ai/) from Zhipu AI (Z.AI).

```bash
zai() {
    (
        export ANTHROPIC_BASE_URL=https://api.z.ai/api/anthropic
        export ANTHROPIC_AUTH_TOKEN=$Z_API_KEY
        claude "$@"
    )
}
```

**Usage:**

```bash
zai                    # start interactive session
zai -p "explain X"     # one-shot prompt
```

</TabItem>
<TabItem label="DeepSeek">

[DeepSeek](https://platform.deepseek.com/) v3.1
chat model.

```bash
dseek() {
    (
        export ANTHROPIC_BASE_URL=https://api.deepseek.com/anthropic
        export ANTHROPIC_AUTH_TOKEN=${DEEPSEEK_API_KEY}
        export ANTHROPIC_MODEL=deepseek-chat
        export ANTHROPIC_SMALL_FAST_MODEL=deepseek-chat
        claude "$@"
    )
}
```

DeepSeek requires explicitly setting
`ANTHROPIC_MODEL` and `ANTHROPIC_SMALL_FAST_MODEL`
because the default Claude model names are not
recognized by the DeepSeek API.

**Usage:**

```bash
dseek                  # start interactive session
dseek -p "explain X"   # one-shot prompt
```

</TabItem>
<TabItem label="MiniMax M2.1">

[MiniMax M2.1](https://platform.minimax.io/docs/guides/text-ai-coding-tools)
from MiniMax.

```bash
ccmm() {
    (
        export ANTHROPIC_BASE_URL=https://api.minimax.io/anthropic
        export ANTHROPIC_AUTH_TOKEN=$MINIMAX_API_KEY
        export API_TIMEOUT_MS=3000000
        export CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1
        export ANTHROPIC_MODEL=MiniMax-M2.1
        export ANTHROPIC_SMALL_FAST_MODEL=MiniMax-M2.1
        export ANTHROPIC_DEFAULT_SONNET_MODEL=MiniMax-M2.1
        export ANTHROPIC_DEFAULT_OPUS_MODEL=MiniMax-M2.1
        export ANTHROPIC_DEFAULT_HAIKU_MODEL=MiniMax-M2.1
        claude "$@"
    )
}
```

MiniMax requires mapping all model slots to
`MiniMax-M2.1` and setting a longer timeout
(`API_TIMEOUT_MS`) since responses can take longer.
Non-essential traffic is disabled to avoid
unnecessary requests.

**Usage:**

```bash
ccmm                   # start interactive session
ccmm -p "explain X"    # one-shot prompt
```

</TabItem>
</Tabs>

## How It Works

Each function:

1. Opens a **subshell** so env vars are scoped
2. Sets `ANTHROPIC_BASE_URL` to the provider's
   Anthropic-compatible endpoint
3. Sets `ANTHROPIC_AUTH_TOKEN` to your API key
4. Optionally overrides model names (required for
   DeepSeek and MiniMax)
5. Launches `claude` with any arguments you pass

Because everything runs in a subshell, your main
shell environment is unaffected. You can have one
terminal running `kimi` and another running `zai`
at the same time.

## See Also

- [Local LLMs](../local-llms/) -- run models locally
  with llama.cpp
- [Chutes Integration](../chutes/) -- use Claude Code
  with the Chutes provider via a router
